---
title: "EM Algorithm"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


The EM algorithm stands for expectation-maximization algorithm. It is a special case of MM(minorization-maximization) algorithm.

# MM Alogrithm {#sec:MM}

Our goal is to solve the problem \@ref(eq:obj),
\begin{equation}
x^* = \arg\max f(x)
(\#eq:obj)
\end{equation}

From its name, the MM algorithm has two steps at each iteration $x_{t}$,

- Minorization: Find a surrogate function $g_t(x), s.t.$

\begin{equation}
g_t(x)\leq f(x),
(\#eq:wh)
\end{equation}

\begin{align}
g_t(x_t) = f(x_t)
(\#eq:loc)
\end{align}

- Maximization: Maximize the surrogate function $g_t(x)$

\begin{equation}
x_{t+1} = \arg\max g_t(x)
(\#eq:max)
\end{equation}

Thus at each iteration, we have 
\begin{equation}
f(x_{t+1})\geq g_t(x_{t+1})\geq g_t(x_t) = f(x_t)
(\#eq:con)
\end{equation}

The inequality \@ref(eq:con) holds because of \@ref(eq:wh), \@ref(eq:max) and \@ref(eq:loc) respectively. 

Since our optimization is on the surrogate function $g_t(x)$, we can avoid dealing with some unpleasant properties in $f(x)$. However, finding such a surrogate function is not easy and does not have universal method that it varys in different problems. 

The EM algorithm can be seen as a special case of MM algorithm. 


# EM Algorithm

The EM algorithm was proposed by @EM. It is widely used dealing with incomlete data set. For example, in the Gaussian mixture regression model(see \@ref(sec:gmm)), the observations are not from homogeneous group. Instead, we assume they come from $K$ several distinct clusters. The problem is we cannot observe their corresponding clusters. What we observe, $y_i$, is in fact conditioned on the clusters, i.e. \begin{equation}
y_i = \mathbf{c}_i | (\mathbf{X} = \mathbf{x}_i,\mathbf{Z}=\mathbf{z}_i)
(\#eq:yi)
\end{equation}
where $\mathbf{c}_i$ is the complete observation which cannot be observed directly, $\mathbf{x}_i$ is the predictor, $\mathbf{z}_i = \left(z_{i 1}, z_{i 2}, \ldots, z_{i m}\right)$ is the missing variable indicating which group $y_i$ belongs to. 

For the following discussions, we always assume $\mathbf{Y}=\mathbf{y}$ is the observed data which is fixed and known from space $\Omega(Y)$, $\mathbf{X} = \mathbf{x}$ is the complete data but not observed from space $\Omega(X)$. Additionally, there exists a many-to-one mapping $\mathbf{x}->\mathbf{y}(\mathbf{x})$ from $\Omega(X)$ to $\Omega(Y)$, and that $\mathbf{x}$ is known only to lie in $M(\mathbf{y})$, the subset determined by the equation $\mathbf{y} = \mathbf{y}(\mathbf{x})$.  We postulate a family of sampling density $f(\mathbf{x}|\Phi)$ depending on parameters $\Phi$ and derive its corresponding family of sampling density $g(\mathbf{y}|\Phi)$ by 
\begin{equation}
g(\mathbf{y}|\Phi) = \int_{M(\mathbf{y})} f(\mathbf{x}|\Phi) d\mathbf{x}
(\#eq:int)
\end{equation}

Our goal is to get the MLE of $\Phi$ from the log-likelihood function $\ell(\Phi|\mathbf{y})$ when an observed data $\mathbf{y}$ is given. The EM algorithm does so by **making essential use of the associated family ** $f(\mathbf{x}|\Phi)$. 

## General EM Algorithm {#sec:gen}

To get the MLE of $\Phi$ from $\ell(\Phi|\mathbf{y})$ given an observed data $\mathbf{y}$, we aim to derive a $\Phi^*$ that maximize $\ell(\Phi|\mathbf{y})$. 

First, $\ell(\Phi|\mathbf{y})$ can be represented by $f(\mathbf{x}|\Phi)$ and a corresponding conditional density $k(\mathbf{x}|\mathbf{y},\Phi)$, i.e.
\begin{align}
(\#eq:conell)
\ell(\Phi|\mathbf{y}) &= \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\mathbf{y},\Phi)\\

k(\mathbf{x}|\mathbf{y},\Phi) &= \frac{f(\mathbf{x}|\Phi)}{g(\mathbf{y}|\Phi)}
\end{align}
Where $k(\mathbf{x}|\mathbf{y},\Phi)$ depends only on $\mathbf{x}$ when $\mathbf{y}$ is given, we denote it as $k(\mathbf{x}|\Phi)$, then \@ref(eq:conell) becomes 

\begin{equation}
\ell(\Phi|\mathbf{y}) = \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\Phi)
(\#eq:ellbefore)
\end{equation}

For \@ref(eq:ellbefore), we integrate (or sum) both sides after multiplying with $k(\mathbf{x}|\Phi^{(t)})$. The LHS has nothing random with  respetive to $\mathbf{x}$. For the RHS, we have 
\begin{align}

\int_{M(\mathbf{y})}\log f(\mathbf{x}|\Phi)k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}
&=E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]:= Q(\Phi|\Phi^{(t)}) \\
\int_{M(\mathbf{y})}\log k(\mathbf{x}|\Phi)k(\mathbf{x}|\Phi^{(t)})&:=H(\Phi|\Phi^{(t)})
\end{align}

Thus we have 
\begin{equation}
\ell(\Phi|\mathbf{y})d\mathbf{x} = Q(\Phi|\Phi^{(t)}) - H(\Phi|\Phi^{(t)})
(\#eq:emfin)
\end{equation}

Where $\Phi^{(t)}$ is the parameter $\Phi$ aftering iterating $t$ times. According to \@ref(eq:emfin), we update $\ell(\Phi|\mathbf{y})$ by repeatedly updating $Q(\Phi|\Phi^{(t)})$ and $H(\Phi|\Phi^{(t)})$. We now give the E- and M-step of EM algorithm.

- E-step: Compute the conditional expectation,
$$
Q(\Phi|\Phi^{(t)}) = E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]
$$ 

- M-step: Derive the next $\Phi^{(t+1)}$ by maximizing $Q(\Phi|\Phi^{(t)})$, i.e. 
\begin{equation}
(\#eq:mgen)
\Phi^{(t+1)} = \arg\max Q(\Phi|\Phi^{(t)})
\end{equation}

Note that $-H(\Phi|\Phi^{(t)})$ can be seen as the entropy of $k(\mathbf{x}|\Phi)$, which is non-negative, since $k(\mathbf{x}|\Phi)\in[0,1]$. 

So $\ell(\Phi|\mathbf{y})\geq Q(\Phi|\Phi')$. Thus $Q(\Phi|\Phi')$ is the minorization of $\ell(\Phi|\mathbf{y})$. Hence the EM algorithm leads to the maximal according to section \@ref(sec:MM). 

Also, we can prove the convergence of EM algorithm by showing that updating $\Phi^{(t)}$ by the E- and M-steps leads to ultimately maximal of $\ell(\Phi|\mathbf{y})$, i.e. we show $\ell(\Phi^{(t+1)}|\mathbf{y})\geq\ell(\Phi^{(t)}|\mathbf{y})$.

\begin{align}
\ell(\Phi^{(t+1)}|\mathbf{y}) - \ell(\Phi^{(t)}|\mathbf{y})
& = \underbrace{(Q(\Phi^{(t+1)}|\Phi^{(t)}) - Q(\Phi^{(t)}|\Phi^{(t)}))}_{\text{M-step, }\geq0}+\underbrace{(H(\Phi^{(t)}|\Phi^{(t)}) - H(\phi^{(t+1)}|\Phi^{(t)}))}_{\text{Jensen's inequality}, f(\cdot) = \log(\cdot),\geq0}\\
&\geq 0
\end{align}


## EM Algorithm for Exponential Distribution

In this section, we restrict the complete density $f(\mathbf{x}|\Phi)$ has regular($\Phi$ is unique up to an arbitary non-singular $r\times r$ linear transformation) exonential-family form
\begin{equation}
f(\mathbf{x}|\Phi) = b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi/a(\Phi)\\
\Phi, \mathbf{t}(\mathbf{x})\in\mathbb{R}^r
\end{equation}
Here, $\mathbf{t}(\mathbf{x})$ is the sufficient statistics. 




### Gaussian Mixture Model {#sec:gmm}

# Acknowledgment {-}

The above contents are notes after taking the class from Prof. @stat5361.

# Reference {-}
