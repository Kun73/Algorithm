---
title: "Distributed Optimization"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

In this note, we summarize three distributed gradient based algorithms solving the problem \@ref(eq:obj), namely *Push-Pull Gradient Method*^[In the original paper, the author considers minimizing the sum of $f_i$, known by agent i only] @pu2018push, *Distributed Stochastic Gradient Tracking Methods(DSGT)* @pu2018distributed, and *Distributed Stochastic Gradient Descent* @olshevsky2019nonasymptotic.

\begin{equation}
\underset{x\in\mathbb{R}^p}{\min} f(x):=\frac{1}{n}\sum_{i=1}^n f_i(x)
(\#eq:obj)
\end{equation}
Where $f_i:\mathbb{R}^p\to \mathbb{R}$ is known by agent $i$ only, and all the agents communicate and exchange information over a network.  

In general, the first two methods use a decision variable $x\in \mathbb{R}^p$ and an auxiliary variable $y\in\mathbb{R}^p$ and have a form of \@ref(eq:unif) while the last one does not introduce an auxiliary variable$y\in\mathbb{R}^p$.

\begin{align}
X_{k+1} &= S_1(X_k-\boldsymbol\alpha Y_k)\\
Y_{k+1} &= S_2Y_k + T(X_{k+1}) - T(X_k)
(\#eq:unif)
\end{align}

Under some conditions, there exists an unique solution to \@ref(eq:obj) $x^*\in\mathbb{R}^{1\times p}$. To prove the convergen of those three methods, the idea is to bound the distance between iterated decision variable and the true value, the distance between iterated decision variable and its own average, and the distance between anxiliary variable and its own average in terms of linear combination of their previous value. This will introduce a matrix $A$.  In order to make it converge, we need to set $\rho(A)<1$, the spectral radius of $A$ to be less than $1$(similar idea in contraction mapping), which will derive a constraint to the stepsize $\alpha$. By doing so, the authors prove the convergence of those three methods and derive their convergence rate. 

# Notations and Assumptions

## Notations
Suppose each agent $i\in\mathcal{N}$ holds a local copy of decision variable $x_i\in\mathbb{R}^p$. Let 
\begin{equation*}
X = (x_1, x_2, ..., x_n)^T\in\mathbb{R}^{n\times p }\\
F(x) := \sum_{i=1}^n f_i(x)\\
\nabla F(x) := (\nabla f_1(x_1), ..., \nabla f_2(x_2))^T\in \mathbb{R}^{n\times p}
\end{equation*}

## Assumptions
(A1) digraph 

# Methods

## Push-Pull Gradient Method

In a digraph, suppose each agent $i$ can actively and reliably push information out to its neighbor  $l\in\mathcal{N}^{out}_{C,i}\subset\mathcal{N}$ and pull information from its neighbor $j\in\mathcal{N}^{in}_{R,i}\subset\mathcal{N}$. To avoid a situation where agent $i$ can only push information or can only pull information from its neighbors, we need assumption *A1*. Matrix $R=(r_{ij})\in\mathbb{R}^{n\times n}$ denotes the pulling weights that agent $i$ pulls information from agent $j$. Thus the row sum of $R$ should be $1$, i.e. $R\boldsymbol 1 = \boldsymbol 1$ and $r_{ij}\geq 0$. That is to say, matrix $R$ is  row-stochastic. Similarly, $C = (c_{ij})\in\mathbb{R}^{n\times n}$ denotes the pushing weights that agent $i$ pushes information to agent $j$. In other words, it denotes the pulling weights that agent $j$ pulls information to agent $i$. Hence $C^T\boldsymbol 1=\boldsymbol 1$, i.e. $\boldsymbol 1^T C=\boldsymbol 1^T, c_{ij}\geq 0$. Moreover, for agent $i$ itself, it will have no problem getting information, hence $r_{ii}>0, c_{ii}>0$. 

The idea of Push-Pull Gradient Methods is that, at each iteration $k$, agent $i$ updates its local copy of decision variable $x_{i,k+1}\in\mathbb R^p$ according to the information it pulls from its nearby agents based on the corresponding pulling weights. Then it will also update the information stored in an auxiliary variable $y_{i, k+1}\in\mathbb{R}^p$ 

---

(Push-Pull Gradient Method)

Each agent $i$ chooses its local step size $\alpha_i\geq0$ and initilized with an arbitary $x_{i,0}\in\mathbb{R}^p, y_{i,0}=\nabla f_i(x_{i,0})$. 

For k = 0, 1, ...,

  For $i\in\mathcal{N}$, 
  
  -$x_{i, k+1} = \sum\limits_{j=1}^nr_{ij}(x_{j, k}-\alpha_j y_{j, k})$ (Pull)
  
  -$y_{i, k+1} = \sum\limits_{j=1}^nc_{ij}y_{j,k}+\nabla f_i(x_{i,k+1})-\nabla f_i(x_{i,k})$(Push)


---

Or in matrix form using $R=(r_{ij})\in\mathbb{R}^{n\times n}, C=(c_{ij})\in\mathbb{R}^{n\times n}, X_k\in\mathbb{R}^{n\times p}, Y_k\in\mathbb{R}^{n\times p}, \boldsymbol\alpha = \text{diag}(\alpha_1,...,\alpha_n)$.
\begin{align}
X_{k+1} &= R(X_{k}-\boldsymbol\alpha Y_k),\\
Y_{k+1} &= CY_k+\nabla F(X_{k+1})-\nabla F(X_k)
(\#eq:pp)
\end{align}


## A Distributed Stochastic Gradient Tracking Method (DSGT)

Now suppose we cannot know exactly what $\nabla f_i(x)$ is. In a system where agents are connected in an undirected graph, suppose each agent $i$ queries a stochastic oracle to obtain noisy gradient samples of the form $g_i(x,\xi_i), x\in\mathbb{R}^p, \xi_i\in\mathbb{R}^m$. Suppose this estimate of gradient is good, which means it is unbiased and has finite second moment. The samples $\xi_i$ are independent and gathered continuouly over time. 

Since we use undirected graph now, if agent $i$ can pull information from its neighbor  agent $j\in\mathcal{N}^{in}_{R,i}$, then it can push information to the same agent $j$. For this situation, we have $\mathcal{N}^{in}_{R,i}=\mathcal{N}^{out}_{C,i}:=\mathcal{N}_{W,i}, W=(w_{ij})\in\mathbb{R}^{n\times n}$. Using $g_i(x_i,\xi_i)$ to estimate $\nabla f_i(x_i)$ instead, we can derive a distributed stochastic gradient tracking method(DSGT) from the Push-Pull method. 

As we have mentioned, $R=C:=W$, then $W$ is doubly stochastic, i.e., $W\boldsymbol1=\boldsymbol1,\boldsymbol1^TW=\boldsymbol1^T$. In addition, $w_{ij}\geq0,w_{ii}>0$. 

--- 

(DSGT)

Choose step siez $\alpha>0$ and initilize each agent $i$ with an arbitary $x_{i,0}\in\mathbb{R}^p, y_{i,0}=\nabla f_i(x_{i,0})$. 

For k = 0, 1, ...,

  For $i\in\mathcal{N}$, 
  
  -$x_{i, k+1} = \sum\limits_{j=1}^nw_{ij}(x_{j, k}-\alpha y_{j, k})$ (Pull)
  
  -$y_{i, k+1} = \sum\limits_{j=1}^nw_{ij}y_{j,k}+ g_i(x_{i,k+1},\xi_{i,k+1})- g_i(x_{i,k},\xi_{i,k})$(Push)

---

Or in matrix form using $W=(w_{ij})\in\mathbb{R}^{n\times n},  X_k\in\mathbb{R}^{n\times p}, Y_k\in\mathbb{R}^{n\times p}, \boldsymbol\alpha = \text{diag}(\alpha,...,\alpha)$.
\begin{align}
X_{k+1} &= W(X_{k}-\boldsymbol\alpha Y_k),\\
Y_{k+1} &= WY_k+G(X_{k+1})-G(X_k)
(\#eq:DSGT)
\end{align}

## A Distributed Stochastic Gradient Descent (DSGD)

Now we directly use the estimate of gradient $g_i(x_i, \xi_i)$ in the update and do not introduce an auxiliary variable. Then,  

\begin{equation}
X_{k+1} = W(X_{k}-\boldsymbol\alpha Y_k)
(\#eq:DSGD)
\end{equation}




# Reference {-}