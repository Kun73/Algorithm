---
title: "EM Algorithm"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

The EM algorithm stands for expectation-maximization algorithm. It is a special case of MM(minorization-maximization) algorithm.

# MM Alogrithm

Our goal is to solve the problem \@ref(eq:obj),
\begin{equation}
x^* = \arg\max f(x)
(\#eq:obj)
\end{equation}

From its name, the MM algorithm has two steps at each iteration $x_{t}$,

- Minorization: Find a surrogate function $g_t(x), s.t.$

\begin{equation}
g_t(x)\leq f(x),
(\#eq:wh)
\end{equation}

\begin{align}
g_t(x_t) = f(x_t)
(\#eq:loc)
\end{align}

- Maximization: Maximize the surrogate function $g_t(x)$

\begin{equation}
x_{t+1} = \arg\max g_t(x)
(\#eq:max)
\end{equation}

Thus at each iteration, we have 
\begin{equation}
f(x_{t+1})\geq g_t(x_{t+1})\geq g_t(x_t) = f(x_t)
(\#eq:con)
\end{equation}

The inequality \@ref(eq:con) holds because of \@ref(eq:wh), \@ref(eq:max) and \@ref(eq:loc) respectively. 

Since our optimization is on the surrogate function $g_t(x)$, we can avoid dealing with some unpleasant properties in $f(x)$. However, finding such a surrogate function is not easy and does not have universal method that it varys in different problems. 

The EM algorithm is a special case when the objective function is the log-likelihood function $\ell(\theta)$. Thus by [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality), we have 
\begin{equation}
f(E(X))\geq E(f(X))
(\#eq:jensen)
\end{equation}


# EM Algorithm

The EM algorithm was proposed by @EM. It is widely used dealing with incomlete data set. For example, in the Gaussian mixture regression model, the observations are not from homogeneous group. Instead, we assume they come from $K$ several distinct clusters. The problem is we cannot observe their corresponding clusters. What we observe, $y_i$, is in fact conditioned on the clusters, i.e. \begin{equation}
y_i = \mathbf{c}_i | (\mathbf{X} = \mathbf{x}_i,\mathbf{Z}=\mathbf{z}_i)
(\#eq:yi)
\end{equation}
where $\mathbf{c}_i$ is the complete observation which cannot be observed directly, $\mathbf{x}_i$ is the predictor, $\mathbf{z}_i = \left(z_{i 1}, z_{i 2}, \ldots, z_{i m}\right)$ is the missing variable indicating which group $y_i$ belongs to. 

For the following discussions, we always assume $\mathbf{Y}=\mathbf{y}$ is the observed data which is fixed and known from space $\Omega(Y)$, $\mathbf{X} = \mathbf{x}$ is the complete data but not observed from space $\Omega(X)$. More specifically, we assume $\mathbf{Y}$ and $\mathbf{X}$ have the following relationship,
\begin{equation}
\mathbf{Y} = \mathbf{X}\ |\ \mathbf{Z}
(\#eq:cond)
\end{equation}
where $\mathbf{Z}$ is unobserved parts.
Additionally, there exists a many-to-one mapping from $\Omega(X)$ to $\Omega(Y)$.  We postulate a family of sampling density $f(\mathbf{x}|\Phi)$ depending on parameters $\Phi$ and derive its corresponding family of sampling density $g(\mathbf{y}|\Phi)$ by 
\begin{equation}
g(\mathbf{y}|\Phi) = \int f(\mathbf{x}|\Phi) d\mathbf{z}
(\#eq:int)
\end{equation}

Our goal is to get the MLE of $\Phi$ from the log-likelihood function $L(\Phi)$ of complete data. We denote the observed log-likelihood function $\ell(\Phi)$ as 
\begin{equation}
\ell(\Phi) = \log g(\mathbf{y}|\Phi)
(\#eq:ell)
\end{equation}

## General EM Algorithm {#sec:gen}

From the above assumptions, the conditional density of unbserved variable $\mathbf{z}$ is 

\begin{equation}
c(\mathbf{z}|\Phi) = \frac{f(\mathbf{x}|\Phi)}{g(\mathbf{y|\Phi})}
(\#eq:dc)
\end{equation}

Then, $\ell(\Phi)$ can be represented by 
\begin{align}
\ell(\Phi) &= \log\frac{f(\mathbf{x}|\Phi)}{c(\mathbf{z}|\Phi)}\\
& = \log f(\mathbf{x}|\Phi) - \log c(\mathbf{z}|\Phi)
(\#eq:ellfc)
\end{align}

Thus
\begin{equation}
L(\Phi) = \log f(\mathbf{x}|\Phi) = \ell(\Phi) + \log c(\mathbf{z}|\Phi)
(\#eq:L)
\end{equation}

- E-step

\begin{align}
E[L(\Phi)] &= E[\ell(\Phi) + \log c(\mathbf{z}|\Phi)]\\
&=\ell(\Phi) + E[\log c(\mathbf{z}|\Phi)]\\
(\#eq:emloc)
\end{align}
The equation \@ref(eq:emloc) is the corresponding part in MM algorithm \@ref(eq:loc). For \@ref(eq:wh) part in MM algorithm, **how to derive?**

- M-step


## EM Algorithm for Exponential Distribution

In this section, we restrict the complete density $f(\mathbf{x}|\Phi)$ has regular($\Phi$ is unique up to an arbitary non-singular $r\times r$ leaner transformation) exonential-family form
\begin{equation}
f(\mathbf{x}|\Phi) = b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi/a(\Phi)\\
\Phi, \mathbf{t}(\mathbf{x})\in\mathbb{R}^r
\end{equation}
Here, $\mathbf{t}(\mathbf{x})$ is the sufficient statistics. 

Generally, we follow the same step in section \@ref(sec:gen). Since the density function is more concrete and restricted, we can get a more specific result.

First, the log-likelihood function of complete-data is 
\begin{equation}
L(\Phi)= \log b(\mathbf{x}) +\mathbf{t}(\mathbf{x})^T\Phi - \log a(\Phi)
(\#eq:Lexp)
\end{equation}

The density of observed data $g(\mathbf{y}|\Phi)$ can be represented by 
\begin{equation}
g(\mathbf{y}|\Phi) = \frac{\int b(\mathbf{x})\exp(\mathbf{t}(\mathbf x)^T\Phi)d\mathbf{z}}{a(\Phi)}
\end{equation}

The conditional density of unobserved part $\mathbf{z}$ is 
\begin{equation}
c(\mathbf{z}|\Phi) = \frac{b(\mathbf{x})\exp(\mathbf{t}(\mathbf x)^T\Phi)}{\int b(\mathbf{x})\exp(\mathbf{t}(\mathbf(x))^T\Phi)d\mathbf{z}}
\end{equation}

Additionally, $f(\mathbf{x}|\Phi)$ is the density function hence $\int f(\mathbf{x|\Phi})d\mathbf{x} = 1$, so we can represent $a(\Phi)$ as 
\begin{equation}
a(\Phi) = \int b(\mathbf{x})\exp(\mathbf{t}(\mathbf x)^T\Phi)d\mathbf{x}
\end{equation}

The integral $\int b(\mathbf{x})\exp(\mathbf{t}(\mathbf(x))^T\Phi)d\mathbf{z}$ can be seen as 
\begin{equation}
a(\Phi|\mathbf{y}) = \int b(\mathbf{x})\exp(\mathbf{t}(\mathbf(x))^T\Phi)d\mathbf{z}
\end{equation}



### Gaussian Mixture Model

# Reference {-}
