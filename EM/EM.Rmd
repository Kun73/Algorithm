---
title: "EM Algorithm"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


The EM algorithm stands for expectation-maximization algorithm. It is a special case of MM(minorization-maximization) algorithm.

# MM Alogrithm {#sec:MM}

Our goal is to solve the problem \@ref(eq:obj),
\begin{equation}
x^* = \arg\max f(x)
(\#eq:obj)
\end{equation}

From its name, the MM algorithm has two steps at each iteration $x_{t}$,

- Minorization: Find a surrogate function $g_t(x), s.t.$

\begin{equation}
g_t(x)\leq f(x),
(\#eq:wh)
\end{equation}

\begin{align}
g_t(x_t) = f(x_t)
(\#eq:loc)
\end{align}

- Maximization: Maximize the surrogate function $g_t(x)$

\begin{equation}
x_{t+1} = \arg\max g_t(x)
(\#eq:max)
\end{equation}

Thus at each iteration, we have 
\begin{equation}
f(x_{t+1})\geq g_t(x_{t+1})\geq g_t(x_t) = f(x_t)
(\#eq:con)
\end{equation}

The inequality \@ref(eq:con) holds because of \@ref(eq:wh), \@ref(eq:max) and \@ref(eq:loc) respectively. 

Since our optimization is on the surrogate function $g_t(x)$, we can avoid dealing with some unpleasant properties in $f(x)$. However, finding such a surrogate function is not easy and does not have universal method that it varys in different problems. 

The EM algorithm can be seen as a special case of MM algorithm. 


# EM Algorithm

The EM algorithm was proposed by @EM. It is widely used dealing with incomlete data set. For example, in the Gaussian mixture regression model(see \@ref(sec:gmm)), the observations are not from homogeneous group. 

For the following discussions, we always assume $\mathbf{Y}=\mathbf{y}$ is the observed data which is fixed and known from space $\Omega(Y)$, $\mathbf{X} = \mathbf{x}$ is the complete data but not observed from space $\Omega(X)$. Additionally, there exists a many-to-one mapping $\mathbf{x}->\mathbf{y}(\mathbf{x})$ from $\Omega(X)$ to $\Omega(Y)$, and that $\mathbf{x}$ is known only to lie in $M(\mathbf{y})$, the subset determined by the equation $\mathbf{y} = \mathbf{y}(\mathbf{x})$.  We postulate a family of sampling density $f(\mathbf{x}|\Phi)$ depending on parameters $\Phi$ and derive its corresponding family of sampling density $g(\mathbf{y}|\Phi)$ by 
\begin{equation}
g(\mathbf{y}|\Phi) = \int_{M(\mathbf{y})} f(\mathbf{x}|\Phi) d\mathbf{x}
(\#eq:int)
\end{equation}

Our goal is to get the MLE of $\Phi$ from the log-likelihood function $\ell(\Phi|\mathbf{y})$ when an observed data $\mathbf{y}$ is given. The EM algorithm does so by **making essential use of the associated family ** $f(\mathbf{x}|\Phi)$. 

## General EM Algorithm {#sec:gen}

To get the MLE of $\Phi$ from $\ell(\Phi|\mathbf{y})$ given an observed data $\mathbf{y}$, we aim to derive a $\Phi^*$ that maximize $\ell(\Phi|\mathbf{y})$. 
First, $\ell(\Phi|\mathbf{y})$ can be represented by $f(\mathbf{x}|\Phi)$ and a corresponding conditional density $k(\mathbf{x}|\mathbf{y},\Phi)$, i.e.
\begin{align}
(\#eq:conell)
\ell(\Phi|\mathbf{y}) &= \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\mathbf{y},\Phi)\\
k(\mathbf{x}|\mathbf{y},\Phi) &= \frac{f(\mathbf{x}|\Phi)}{g(\mathbf{y}|\Phi)}
\end{align}
Where $k(\mathbf{x}|\mathbf{y},\Phi)$ depends only on $\mathbf{x}$ when $\mathbf{y}$ is given, we denote it as $k(\mathbf{x}|\Phi)$, then \@ref(eq:conell) becomes 
\begin{equation}
\ell(\Phi|\mathbf{y}) = \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\Phi)
(\#eq:ellbefore)
\end{equation}
For \@ref(eq:ellbefore), we integrate (or sum) both sides after multiplying with $k(\mathbf{x}|\Phi^{(t)})$. The LHS has nothing random with  respetive to $\mathbf{x}$. For the RHS, we have 
\begin{align}
\int_{M(\mathbf{y})}\log (f(\mathbf{x}|\Phi))k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}
&=E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]:= Q(\Phi|\Phi^{(t)}) \\
\int_{M(\mathbf{y})}\log( k(\mathbf{x}|\Phi))k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}&:=H(\Phi|\Phi^{(t)})
\end{align}
Thus we have 
\begin{equation}
\ell(\Phi|\mathbf{y}) = Q(\Phi|\Phi^{(t)}) - H(\Phi|\Phi^{(t)})
(\#eq:emfin)
\end{equation}
Where $\Phi^{(t)}$ is the parameter $\Phi$ aftering iterating $t$ times. According to \@ref(eq:emfin), we update $\ell(\Phi|\mathbf{y})$ by repeatedly updating $Q(\Phi|\Phi^{(t)})$ and $H(\Phi|\Phi^{(t)})$. We now give the E- and M-step of EM algorithm.
- E-step: Compute the conditional expectation,
$$
Q(\Phi|\Phi^{(t)}) = E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]
$$ 
- M-step: Derive the next $\Phi^{(t+1)}$ by maximizing $Q(\Phi|\Phi^{(t)})$, i.e. 
\begin{equation}
(\#eq:mgen)
\Phi^{(t+1)} = \arg\max Q(\Phi|\Phi^{(t)})
\end{equation}
Note that $-H(\Phi|\Phi^{(t)})$ can be seen as the entropy of $k(\mathbf{x}|\Phi)$, which is non-negative, since $k(\mathbf{x}|\Phi)\in[0,1]$. 
So $\ell(\Phi|\mathbf{y})\geq Q(\Phi|\Phi')$. Thus $Q(\Phi|\Phi')$ is the minorization of $\ell(\Phi|\mathbf{y})$. Hence the EM algorithm leads to the maximal according to section \@ref(sec:MM). 
Also, we can prove the convergence of EM algorithm by showing that updating $\Phi^{(t)}$ by the E- and M-steps leads to ultimately maximal of $\ell(\Phi|\mathbf{y})$, i.e. we show $\ell(\Phi^{(t+1)}|\mathbf{y})\geq\ell(\Phi^{(t)}|\mathbf{y})$.
\begin{align}
\ell(\Phi^{(t+1)}|\mathbf{y}) - \ell(\Phi^{(t)}|\mathbf{y})
& = \underbrace{(Q(\Phi^{(t+1)}|\Phi^{(t)}) - Q(\Phi^{(t)}|\Phi^{(t)}))}_{\text{M-step, }\geq0}+\underbrace{(H(\Phi^{(t)}|\Phi^{(t)}) - H(\phi^{(t+1)}|\Phi^{(t)}))}_{\text{Jensen's inequality}, f(\cdot) = \log(\cdot),\geq0}\\
&\geq 0
\end{align}
## EM Algorithm for Exponential Distribution
In this section, we restrict the complete density $f(\mathbf{x}|\Phi)$ has regular($\Phi$ is unique up to an arbitary non-singular $r\times r$ linear transformation) exonential-family form
\begin{equation}
(\#eq:exp)
f(\mathbf{x}|\Phi) = b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi/a(\Phi)\\
\Phi, \mathbf{t}(\mathbf{x})\in\mathbb{R}^r
\end{equation}
Here, $\mathbf{t}(\mathbf{x})$ is the sufficient statistics. 
From the above discussion \@ref(eq:emfin),Substituting $f(\mathbf{x}|\Phi)$, we have 
\begin{align}
k(\mathbf{x}|\mathbf{y},\Phi) = \frac{b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi)}{\int_{M(\mathbf{y})} b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi d\mathbf{x}}
:=\frac{b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi)}{a(\Phi|\mathbf{y})}
\end{align}
Thus 
\begin{align}
Q(\Phi|\Phi^{(t)}) &= \int_{M(y)}\log b(\mathbf{x})k(\mathbf{x}|\Phi^{(t)})d\mathbf{x} + \int_{M(y)}\mathbf{t}(\mathbf{x})^T\Phi k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}-\int_{M(y)} \log (a(\Phi)) k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}\\
&:=B(\Phi^{(t)}) + \Phi^T E[\mathbf{t}|\Phi^{(t)},\mathbf{y}] - \log a(\Phi)
\end{align}
Let $Q'(\Phi|\Phi^{(t)})=E[\mathbf{t}|\Phi^{(t)},\mathbf{y}]-\frac{a'(\Phi)}{a(\Phi)} = 0$, notice that $\int f(\mathbf{x}|\Phi)d\mathbf{x}=1$, then
\begin{align}
a(\Phi) &= \int b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x} \stackrel{\text{take ln then take derivative}}\Rightarrow\\
\frac{a'(\Phi)}{a(\Phi)} &= \frac{\int b(\mathbf{x}) \mathbf{t}(\mathbf{x})^T \exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x}}{\int b(\mathbf{x})  \exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x}}
= E[\mathbf{t}(\mathbf{x})|\Phi,\mathbf{y}]
\end{align}
Hence we derive the EM algorithm for exponential family as follows
- E-step: Compute $E[\mathbf{t}(\mathbf{x})|\phi^{(t)},\mathbf{y}]$
- M-step: Update $\Phi^{(t+1)}$ by solving $E[\mathbf{t}(\mathbf{x})|\phi^{(t+1)},\mathbf{y}]=E[\mathbf{t}(\mathbf{x})|\phi^{(t)},\mathbf{y}]$

## Gaussian Mixture Model {#sec:gmm}

For a Gaussian mixture model, we have 
\begin{equation}
y_i = \mathbf{c}_i | (\mathbf{X} = \mathbf{x}_i,\mathbf{Z}=\mathbf{z}_i)
(\#eq:gmmdata)
\end{equation}
where $\mathbf{c}_i$ is the complete observation which cannot be observed directly, $\mathbf{x}_i$ is the predictor, $\mathbf{z}_i = \left(z_{i 1}, z_{i 2}, \ldots, z_{i m}\right)$ is the latent variable indicating which group $y_i$ belongs to. 
Suppose the density of $y_i$ is given by 
\begin{equation}
f\left(y_{i} | \mathbf{x}_{i}, \mathbf{\Phi}\right)=\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i} ; \mathbf{x}_{i}^{\top} \boldsymbol{\beta}_{j}, \sigma^{2}\right), \quad i=1, \ldots, n
(\#eq:gmmden)
\end{equation}
Where $\pi_j's$ are called mixing parameters, $\boldsymbol{\beta}_j$ is the regression coefficient vector for the $j$th group, $\phi\left(y_{i} ; \mu, \sigma^{2}\right)$  denotes the density function of $\mathcal{N}(\mu,\sigma^2)$, $\boldsymbol{\Phi}=(\pi_1,\boldsymbol{\beta}_1,...,\pi_m,\boldsymbol{\beta}_m,\sigma)$ contains all the unknown parameters.
The log-likelihood function $\ell(\boldsymbol{\Phi})$ of observed data can be represented as 
\begin{equation}
\ell(\boldsymbol{\Phi}) = \sum_{i=1}^n\log \left[\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i} ; \mathbf{x}_{i}^{\top} \boldsymbol{\beta}_{j}, \sigma^{2}\right)\right]
(\#eq:gmmell)
\end{equation}

### E-step

For the $t$th iteration, define $Q(\Psi|\Psi^{(t)})$ as 

\begin{align}
Q(\Psi|\Psi^{(t)}) &= E[\log f(\mathbf{x}, \mathbf{z},y|\Psi)|y,\mathbf{x},\Psi^{(t)}]\\
&= \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},y,\Psi)\log \log f(\mathbf{x}, \mathbf{z}_{ij}=j,y|\Psi)\\
&= \underbrace{\sum_{i=1}^n\sum_{j=1}^m p(z_{ij} = j|\mathbf{x}_i,y_i,\Psi^{(t)})}_{p_{ij}^{(t+1)}} \left[\pi_j\phi(y_i;\mathbf{x}_i^T\beta_j^{t},\sigma^{2^{(t)}})\right]
\end{align}

where,
$$
p_{ij}^{(t+1)} = \frac{p(z_{ij}=j,\mathbf{x}_i|\Psi^{(t)})}{p(\mathbf{x}_i,y|\Psi^{(t)})}= \frac{\pi_j^{(t)}\phi(y_i;\mathbf{x}_i\beta_j^{(t)},\sigma^{2^{(t)}})}{\sum_{j=1}^m\pi_j^{(t)}\phi(y_i;\mathbf{x}_i\beta_j^{(t)},\sigma^{2^{(t)}})}
$$

For this example, $Q(\Psi|\Psi^{(t)})$ can be written more concrete as 
\begin{align}
Q(\Psi|\Psi^{(t)}) 
&= \sum_{i=1}^n\sum_{j=1}^m p_{ij}^{(t+1)}\ln((2\pi)^{-1/2}\pi_{j}) - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^mp_{ij}^{(t+1)}\ln\sigma^{2^{(t)}} -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^mp_{ij}^{(t+1)}(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})^T(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})/\sigma^{2^{(t)}}\\
&=I_1-\frac{I_2}{2}-\frac{I_3}{2}
(\#eq:Q)
\end{align}

For $\beta_j$, only $I_{3j}=\sum_{i=1}^np_{ij}^{(t+1)}(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})^T(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})/\sigma^{2^{(t)}}$ in \@ref(eq:Q) involves it, let $\frac{\partial}{\partial\beta_j}\frac{I_{3j}}{2} = 0$, we have 

\begin{equation}
-\sum_{i=1}^n\frac{p_{ij}^{(t+1)}}{\sigma^{2^{(t)}}}\mathbf{x}_i(y_i-\mathbf{x}_i^T\beta_j)=0
(\#eq:betaj)
\end{equation}

Solve the equation \@ref(eq:betaj), we have 
\begin{equation}
\boldsymbol{\beta}_{j}^{(t+1)}=\left(\sum_{i=1}^{n} \mathbf{x}_{i} \mathbf{x}_{i}^{\top} p_{i j}^{(t+1)}\right)^{-1}\left(\sum_{i=1}^{n} \mathbf{x}_{i} p_{i j}^{(t+1)} y_{i}\right), \quad j=1, \ldots, m
\end{equation}

For $\sigma^{2^{(t)}}$, only $I_2,I_3$ in \@ref(eq:Q) invlove it. Similarly, we take the partial derivative with respect to $\sigma^{2^{(t)}}$, we have 
\begin{equation}
-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^m\frac{p_{ij}^{(t+1)}}{\sigma^{2^{(t)}}}+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^m\frac{p_{ij}^{(t+1)}(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})^T(y_i-\mathbf{x}_i^T\beta_{j}^{(t)})}{\sigma^{4^{(t)}}} = 0
(\#eq:sigma)
\end{equation}

Solve the equation \@ref(eq:sigma) and notice that $\sum_{j=1}^mp_{ij}^{(t+1)}=1$, we derive the fomula to update $\sigma^{2^{(t)}}$.
\begin{equation}
\sigma^{2^{(t+1)}} = \frac{\sum_{i=1}^{n}\sum_{j=1}^{m} p_{i j}^{(t+1)}(y_i-\mathbf{x}_i^T\beta_j^{(t+1)})^2}{n}
\end{equation}

### M-step

Maximize $Q(\Psi|\Psi^{(t)})$ under the constraint $\sum_{j=1}^m\pi_j = 1$, we use the method of Lagrange multipliers. Let $L(\Psi,\lambda|\Psi^{(t)})$ be 
\begin{equation}
L(\Psi,\lambda|\Psi^{(t)}) = Q(\Psi|\Psi^{(t)})-\lambda (\sum_{j=1}^m\pi_j-1)
(\#eq:lagrange)
\end{equation}

Let 
$$
L'(\Psi,\lambda|\Psi^{(t)}) = 0
$$
For $\pi_j$, we have 
\begin{equation}
\sum_{i=1}^n\frac{p_{ij}^{(t+1)}}{\pi_j}-\lambda = 0
(\#eq:pij)
\end{equation}

Notice that $\sum_{j=1}^m\pi_j=1, \sum_{j=1}^m p_{ij}^{(t+1)}=1$, take summation of $i$ in \@ref(eq:pij), we get 
$\lambda = n$. Thus we derive the formula to update $\pi_j$ as 
\begin{equation}
\pi_j^{(t+1)} = \frac{\sum_{i=1}^n p_{ij}^{(t+1)}}{n}
(\#eq:uppi)
\end{equation}

For $\beta_j$, we 

### Implemented Function

The function `em_update()` is for each E- and M-step, the function `regmix_em()` is to iterates EM-algorithm untile convergence or up to maximum iteration times.

```{r}
em_update <- function(y, xmat, PI, bt, sgm) {
  n <- dim(xmat)[1]
  m <- length(PI)
  log_phi <- matrix(1:m * n, ncol = m, nrow = n)
  log_phi <- dnorm(y - xmat %*% bt, 0, sqrt(sgm))
  pij <- matrix(1:m * n, ncol = m, nrow = n)
  ## avoid iterating on i
  for (j in 1:m) {
    pij[, j] <- (PI[j] * log_phi[, j]) / (log_phi %*% PI)
  }
  for (j in 1:m) {
    xsp <- sqrt(pij[, j]) * xmat
    xp <- pij[, j] * xmat
    bt[, j] <- solve(t(xsp) %*% xsp) %*% (t(xp) %*% y)
  }
  PI <- colSums(pij) / n
  sgm <- sum(pij * ((y - xmat %*% bt)**2)) / n
  list(PI = PI, bt = bt, sgm = sgm)
}

```

```{r}
regmix_em <- function (y, xmat, pi.init, beta.init, sigma.init, control) {
  
  stop <- 1
  convergence <- FALSE
  tol <- control$tol
  maxit <- control$maxit
  bt <- beta.init
  PI <- pi.init
  sgm <- sigma.init
  
  for (i in 1 : maxit) {
    if (stop >= tol) {
      PHI_n <- em_update(y, xmat, PI, bt, sgm)
      C <- sum(PI**2) + sum(bt**2) +sum(sgm**2)
      stop <- (sum((PHI_n$PI - PI)**2) + sum((PHI_n$bt - bt)**2) 
               + sum((PHI_n$sgm - sgm)**2)) / (C + 0.01)
      PI <- PHI_n$PI
      bt <- PHI_n$bt
      sgm <- PHI_n$sgm
    } else{
      convergence <- TRUE
      return (list(PHI = PHI_n, convergence = convergence, iteration = i))
    }
  }
  list(PHI = PHI_n, convergence = convergence, iteration = maxit)
}
```

#### A Simulated Example

```{r}
regmix_sim <- function(n, pi, beta, sigma) {
    K <- ncol(beta)
    p <- NROW(beta)
    xmat <- matrix(rnorm(n * p), n, p) # normal covaraites
    error <- matrix(rnorm(n * K, sd = sigma), n, K)
    ymat <- xmat %*% beta + error # n by K matrix
    ind <- t(rmultinom(n, size = 1, prob = pi))
    y <- rowSums(ymat * ind)
    data.frame(y, xmat)
}
n <- 400
pi <- c(.3, .4, .3)
bet <- matrix(c( 1,  1,  1, 
                 -1,  -1,  -1), 2, 3)
sig <- 1
set.seed(1205)
dat <- regmix_sim(n, pi, bet, sig)
xmat <- as.matrix(dat[, -1], ncol = 3, nrow = n)
control <- list(maxit = 500, tol = 1e-8)
bt <- matrix(rnorm(6), nrow = 2, ncol = 3)

PHI <- regmix_em(dat$y, xmat, pi / pi / length(pi), bt, sig / sig, control) 
```

The result is
```{r}
PHI
```
Where `PI` stands for $\boldsymbol\pi$, `bt` represents for $\boldsymbol\beta$ and `sqm` stands for $\sigma^2$.





## Variants of EM

### Monte Carlo EM(MCEM)

### Generalized EM(GEM)

## Acceleration of EM


# Acknowledgment {-}

The above contents are notes after taking the class from Prof. @stat5361.

# Reference {-}
