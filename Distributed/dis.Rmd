---
title: "Distributed Optimization"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

In this note, we summarize three distributed gradient based algorithms solving the problem \@ref(eq:obj), namely *Push-Pull Gradient Method*^[In the original paper, the author considers minimizing the sum of $f_i$, known by agent i only] @pu2018push, *Distributed Stochastic Gradient Tracking Methods(DSGT)* @pu2018distributed, and *Distributed Stochastic Gradient Descent* @olshevsky2019nonasymptotic.

\begin{equation}
\underset{x\in\mathbb{R}^p}{\min} f(x):=\frac{1}{n}\sum_{i=1}^n f_i(x)
(\#eq:obj)
\end{equation}
Where $f_i:\mathbb{R}^p\to \mathbb{R}$ is known by agent $i$ only, and all the agents communicate and exchange information over a network.  

In general, the first two methods use a decision variable $x\in \mathbb{R}^p$ and an auxiliary variable $y\in\mathbb{R}^p$ and have a form of \@ref(eq:unif) while the last one does not introduce an auxiliary variable$y\in\mathbb{R}^p$.

\begin{align}
X_{k+1} &= S_1(X_k-\boldsymbol\alpha Y_k)\\
Y_{k+1} &= S_2Y_k + T(X_{k+1}) - T(X_k)
(\#eq:unif)
\end{align}

Under some conditions, there exists an unique solution to \@ref(eq:obj) $x^*\in\mathbb{R}^{1\times p}$. To prove the convergence of those three methods, the idea is to bound the distance between iterated decision variable and the true value, the distance between iterated decision variable and its own average, and the distance between anxiliary variable and its own average in terms of linear combination of their previous value. This will introduce a matrix $A$.  In order to make it converge, we need to set $\rho(A)<1$, the spectral radius of $A$ to be less than $1$(similar idea in contraction mapping), which will derive a constraint to the stepsize $\alpha$. By doing so, the authors prove the convergence of those three methods and derive their convergence rate. 

# Notations and Assumptions

## Notations
Suppose each agent $i\in\mathcal{N}$ holds a local copy of decision variable $x_i\in\mathbb{R}^p$. Let 
\begin{equation*}
X = (x_1, x_2, ..., x_n)^T\in\mathbb{R}^{n\times p }\\
F(x) := \sum_{i=1}^n f_i(x)\\
\nabla F(x) := (\nabla f_1(x_1), ..., \nabla f_2(x_2))^T\in \mathbb{R}^{n\times p}
\end{equation*}

## Assumptions
(A1) digraph 

# Methods

## Push-Pull Gradient Method

In a digraph, suppose each agent $i$ can actively and reliably push information out to its neighbor  $l\in\mathcal{N}^{out}_{C,i}\subset\mathcal{N}$ and pull information from its neighbor $j\in\mathcal{N}^{in}_{R,i}\subset\mathcal{N}$. To avoid a situation where agent $i$ can only push information or can only pull information from its neighbors, we need assumption *A1*. Matrix $R=(r_{ij})\in\mathbb{R}^{n\times n}$ denotes the pulling weights that agent $i$ pulls information from agent $j$. Thus the row sum of $R$ should be $1$, i.e. $R\boldsymbol 1 = \boldsymbol 1$ and $r_{ij}\geq 0$. That is to say, matrix $R$ is  row-stochastic. Similarly, $C = (c_{ij})\in\mathbb{R}^{n\times n}$ denotes the pushing weights that agent $i$ pushes information to agent $j$. In other words, it denotes the pulling weights that agent $j$ pulls information to agent $i$. Hence $C^T\boldsymbol 1=\boldsymbol 1$, i.e. $\boldsymbol 1^T C=\boldsymbol 1^T, c_{ij}\geq 0$. Moreover, for agent $i$ itself, it will have no problem getting information, hence $r_{ii}>0, c_{ii}>0$. 

Hence we have assumption on $R$ and $C$, 


The idea of Push-Pull Gradient Methods is that, at each iteration $k$, agent $i$ updates its local copy of decision variable $x_{i,k+1}\in\mathbb R^p$ according to the information it pulls from its nearby agents based on the corresponding pulling weights. Then it will also update the information stored in an auxiliary variable $y_{i, k+1}\in\mathbb{R}^p$ 

---

(Push-Pull Gradient Method)

Each agent $i$ chooses its local step size $\alpha_i\geq0$ and initilized with an arbitary $x_{i,0}\in\mathbb{R}^p, y_{i,0}=\nabla f_i(x_{i,0})$. 

For k = 0, 1, ...,

  * For $i\in\mathcal{N}$, 
  
    * $x_{i, k+1} = \sum\limits_{j=1}^nr_{ij}(x_{j, k}-\alpha_j y_{j, k})$ (Pull)
  
    * $y_{i, k+1} = \sum\limits_{j=1}^nc_{ij}y_{j,k}+\nabla f_i(x_{i,k+1})-\nabla f_i(x_{i,k})$(Push)


---

Or in matrix form using $R=(r_{ij})\in\mathbb{R}^{n\times n}, C=(c_{ij})\in\mathbb{R}^{n\times n}, X_k\in\mathbb{R}^{n\times p}, Y_k\in\mathbb{R}^{n\times p}, \boldsymbol\alpha = \text{diag}(\alpha_1,...,\alpha_n)$.
\begin{align}
X_{k+1} &= R(X_{k}-\boldsymbol\alpha Y_k),\\
Y_{k+1} &= CY_k+\nabla F(X_{k+1})-\nabla F(X_k)
(\#eq:pp)
\end{align}


## A Distributed Stochastic Gradient Tracking Method (DSGT)

Now suppose we cannot know exactly what $\nabla f_i(x)$ is. In a system where agents are connected in an undirected graph, suppose each agent $i$ queries a stochastic oracle to obtain noisy gradient samples of the form $g_i(x,\xi_i), x\in\mathbb{R}^p, \xi_i\in\mathbb{R}^m$. Suppose this estimate of gradient is good, which means it is unbiased and has finite second moment. The samples $\xi_i$ are independent and gathered continuouly over time. 

Since we use undirected graph now, if agent $i$ can pull information from its neighbor  agent $j\in\mathcal{N}^{in}_{R,i}$, then it can push information to the same agent $j$. For this situation, we have $\mathcal{N}^{in}_{R,i}=\mathcal{N}^{out}_{C,i}:=\mathcal{N}_{W,i}, W=(w_{ij})\in\mathbb{R}^{n\times n}$. Using $g_i(x_i,\xi_i)$ to estimate $\nabla f_i(x_i)$ instead, we can derive a distributed stochastic gradient tracking method(DSGT) from the Push-Pull method. 

As we have mentioned, $R=C:=W$, then $W$ is doubly stochastic, i.e., $W\boldsymbol1=\boldsymbol1,\boldsymbol1^TW=\boldsymbol1^T$. In addition, $w_{ij}\geq0,w_{ii}>0$. 

--- 

(DSGT)

Choose step siez $\alpha>0$ and initilize each agent $i$ with an arbitary $x_{i,0}\in\mathbb{R}^p, y_{i,0}=\nabla f_i(x_{i,0})$. 

For k = 0, 1, ...,

  * For $i\in\mathcal{N}$, 
  
    * $x_{i, k+1} = \sum\limits_{j=1}^nw_{ij}(x_{j, k}-\alpha y_{j, k})$ (Pull)
  
    * $y_{i, k+1} = \sum\limits_{j=1}^nw_{ij}y_{j,k}+ g_i(x_{i,k+1},\xi_{i,k+1})- g_i(x_{i,k},\xi_{i,k})$(Push)

---

Or in matrix form using $W=(w_{ij})\in\mathbb{R}^{n\times n},  X_k\in\mathbb{R}^{n\times p}, Y_k\in\mathbb{R}^{n\times p}, \boldsymbol\alpha = \text{diag}(\alpha,...,\alpha)$.
\begin{align}
X_{k+1} &= W(X_{k}-\boldsymbol\alpha Y_k),\\
Y_{k+1} &= WY_k+G(X_{k+1})-G(X_k)
(\#eq:DSGT)
\end{align}

## A Distributed Stochastic Gradient Descent (DSGD)

Now we directly use the estimate of gradient $g_i(x_i, \xi_i)$ in the update and do not introduce an auxiliary variable. Then,  

\begin{equation}
X_{k+1} = W(X_{k}-\boldsymbol\alpha Y_k)
(\#eq:DSGD)
\end{equation}

# Convergence Analysis

To prove the convergence of thoses three methods, we bound the three distance. 


## Push-Pull

The goal is to bound $\Vert\bar x_{k+1}-x^*\Vert_2, \Vert X_{k+1}-\boldsymbol1\bar x_{k+1}\Vert_R$,and $\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C$. Where $\Vert\cdot\Vert_R$ and $\Vert\cdot\Vert_C$ are some defined norms,

```{remark, n1}
$X_{k+1}-\boldsymbol1\bar x_{k+1}$ and $Y_{k+1}-v\bar y_{k+1}$ are both $\mathbb{R}^{n\times p}$, the definition of matrix norm given an arbitary vector norm used here is different from the usual matrix norm induced by a vector norm.
```



### Relationship between two iteration steps

Recall in Push-Pull method, we have \@ref(eq:pp), which involves two matrices $R$ and $C$ containing pulling weights and pushing weights respectively. By adding assumptions on the graph $\mathcal{G}_R$ and $\mathcal{G}_{C^T}$ and the stochastic attributes of $R$ and $C$, we have 

```{lemma, eigvrc}
Under assumptions, the matrix $R$ has a unique nonnegative left eigenvector $u^T$(w.r.t. eigenvalue 1) with $u^T\boldsymbol1 = n$, and the matrix $C$ has a unique nonnegative right eigenvector $v$ (w.r.t. eigenvalue 1) with $\boldsymbol1^T v = n$, i.e., 
$$
  u^T R = 1\cdot u^T
$$

$$
  Cv = 1\cdot v
$$
  
Moreover, $u^T$ (resp., $v$) is nonzero only on the entries associated with agents $i\in\mathcal{R}_R$(resp., $j\in\mathcal{R}_{C^T}$), and $u^Tv>0$.

```

Next, we give definition of $\bar x_k$ and $\bar y_k$.

\begin{equation}
\bar x_k := \frac{1}{n}u^TX_k\in\mathbb{R}^{1\times p},\quad \bar y_k:= \frac{1}{n}\boldsymbol 1 \nabla F(X_k)\in\mathbb{R}^{1\times p}
(\#eq:barpp)
\end{equation}

The authors do not define $\bar x_k$ as $\frac{1}{n}\boldsymbol 1^TX_k$ is because the pulling information is subject to the pulling weights matrix $R$, not the whole system.

For the pull step, 

\begin{equation}
\bar x_{k+1} = \frac{1}{n}u^TX_{k+1}\stackrel{\text{pull step}}{=}\frac{1}{n}u^TR(X_k-\boldsymbol\alpha Y_k)=\bar x_k-\frac{1}{n}u^T\boldsymbol\alpha Y_k
(\#eq:barxpp)
\end{equation}

Hence,

\begin{align}

X_{k+1}-\boldsymbol1\bar x_{k+1}&= R(X_k-\boldsymbol\alpha Y_k)-\boldsymbol1(\bar x_k-\frac{1}{n}u^T\boldsymbol\alpha Y_k)\\
&=(R-\frac{\boldsymbol 1 u^T}{n})(X_k-\boldsymbol 1\bar x_k)-
(R-\frac{\boldsymbol 1 u^T}{n})\boldsymbol\alpha Y_k+\frac{\boldsymbol 1 u^T}{n}(X_k-\boldsymbol 1\bar x_k)\\
&=(R-\frac{\boldsymbol 1 u^T}{n})(X_k-\boldsymbol 1\bar x_k)-
(R-\frac{\boldsymbol 1 u^T}{n})\boldsymbol\alpha Y_k
(\#eq:pp1bar)

\end{align}
This is because $\frac{\boldsymbol 1 u^T}{n}(X_k-\boldsymbol 1\bar x_k)=\bar x_k-\frac{u^T\boldsymbol1}{n}\bar x_k=0$ according to lemma \@ref(lem:eigvrc). 

To see what does this difference mean, we rewrite $X_{k+1}-\boldsymbol1\bar x_{k+1}$ as 

\begin{equation}
(x_1-\frac{1}{n}\sum_{i=1}^n u_i x_i,...,x_n-\frac{1}{n}\sum_{i=1}^n u_i x_i)^T\in\mathbb{R}^{n\times p}
\end{equation}
where $u=(u_1,...,u_n)^T\in\mathbb{R}^n,x_i\in\mathbb{R}^p$. It denotes the difference between each agent $i$'s decision variable and the overall weighted mean.

```{remark}
The interpretation of $R-\frac{\boldsymbol 1 u^T}{n}$?
```

For the push step, 

\begin{align}
Y_{k+1}-v\bar y_{k+1}&\stackrel{\text{push step}}=CY_k+\nabla F(X_{k+1})-\nabla F(X_k)-v[\frac{1}{n}\boldsymbol1(CY_k+\nabla F(X_{k+1})-\nabla F(X_k))]\\
&=CY_k-v\bar y_k+(I-\frac{v\boldsymbol1^T}{n})(\nabla F(X_{k+1})-\nabla F(X_k))\\
&=(C-\frac{v\boldsymbol1^T}{n})(Y_k-v\bar y_k)+(I-\frac{v\boldsymbol1^T}{n})(\nabla F(X_{k+1})-\nabla F(X_k))+Cv\bar y_k+\frac{v\boldsymbol1^T}{n}Y_k-\frac{v\boldsymbol1^Tv}{n}\bar y_k-v\bar y_k\\
&=(C-\frac{v\boldsymbol1^T}{n})(Y_k-v\bar y_k)+(I-\frac{v\boldsymbol1^T}{n})(\nabla F(X_{k+1})-\nabla F(X_k))
(\#eq:ppvbar)
\end{align}
where 
$$
\frac{1}{n}v\boldsymbol1^TCY_k=v\bar y_k
$$

$$
Cv\bar y_k+\frac{v\boldsymbol1^T}{n}Y_k-\frac{v\boldsymbol1^Tv}{n}\bar y_k-v\bar y_k=v\bar y_k+v\bar y_k-v\bar y_k-v\bar y_k=0
$$
This is because the column-stochastic of $C$ and lemma \@ref(lem:eigvrc).

Similarly, we can rewrite $Y_{k+1}-v\bar y_{k+1}$ as 
\begin{equation}
(y_1-\frac{1}{n}v_1\sum_{i=1}^ny_i,...,y_n-\frac{1}{n}v_n\sum_{i=1}^ny_i)^T
\end{equation}

Where $v=(v_1,...,v_n)^T\in\mathbb{R}^n,y_i\in\mathbb R^p$. It denotes the difference between each agent $i$'s average gradient and the overall mean subjected to its pushed situation with other agents.

Additionally, recall our goal is to bound those three distance, from \@ref(eq:barxpp), we separate $Y_k$ as $Y_k-v\bar y_k$ and $v\bar y_k$, then 

\begin{align}

\bar x_{k+1} &=\bar x_k -\underbrace{\frac{1}{n}u^T\boldsymbol\alpha v}_{\alpha'}\bar y_k-\frac{1}{n}u^T\boldsymbol \alpha(Y_k-v\bar y_k)\\
&=\bar x_k-\alpha'(\bar y_k-\underbrace{\frac{1}{n}\boldsymbol 1^T\nabla F(\boldsymbol 1\bar x_k)}_{g_k})-\frac{1}{n}\alpha'\boldsymbol 1^T\nabla F(\boldsymbol 1\bar x_k)-frac{1}{n}u^T\boldsymbol\alpha(Y_k-v\bar y_k)\\
&=\bar x_k-\alpha'(\bar y_k-g_k)-\alpha'g_k-\frac{1}{n}u^T\boldsymbol\alpha(Y_k-v\bar y_k)
(\#eq:xbar2pp)
\end{align}

The auther introduce $g_k=\frac{1}{n}\boldsymbol 1^T\nabla F(\boldsymbol 1\bar x_k)$ is because we can prove
\begin{equation}
\bar y_k =\frac{1}{n}\boldsymbol 1^T \nabla F(X_k)
(\#eq:baryknabla)
\end{equation}

### Linear system of inequalities

we then bound $(\Vert\bar x_{k+1}-x^*\Vert_2, \Vert X_{k+1}-\boldsymbol1\bar x_{k+1}\Vert_R,\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C)^T$ by linear system of inequalities. 


```{lemma, eapp}
Under some conditions, $\exists A\in \mathbb R^{3\times 3}, s.t.$
\begin{equation}
\left(
\begin{array}{c}
\Vert\bar x_{k+1}-x^*\Vert_2,\\
\Vert X_{k+1}-\boldsymbol1\bar x_{k+1}\Vert_R,\\
\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C)^T
\end{array}
\right)
\leq A 
\left(
\begin{array}{c}
\Vert\bar x_{k}-x^*\Vert_2,\\
\Vert X_{k}-\boldsymbol1\bar x_{k}\Vert_R,\\
\Vert Y_{k}-v\bar y_{k}\Vert_C
\end{array}
\right)
(\#eq:ineqpp)
\end{equation}
```

Next we derive the elements of $A$. We add supported lemmas in derivation.

First, for $\Vert\bar x_{k+1}-x^*\Vert_2$, substitute $\bar x_{k+1}$ using \@ref(eq:xbar2pp), we have 

\begin{align}

\Vert\bar x_{k+1}-x^*\Vert_2&\leq \left\|\bar{x}_{k}-\alpha^{\prime} g_{k}-x^{*}\right\|_{2}+\alpha^{\prime}\left\|\bar{y}_{k}-g_{k}\right\|_{2}+\frac{1}{n}\left\|u^{\top} \boldsymbol{\alpha}\left(Y_{k}-v \bar{y}_{k}\right)\right\|_{2}
(\#eq:ineq11)
\end{align}

On the right hand side, $\Vert \bar{x}_{k}-\alpha^{\prime} g_{k}-x^{*}\Vert_2$ is the distance between the optimal and iterated value, $\vert \bar{y}_{k}-g_{k}\Vert_2$ is the distance between average gradient and gradient of iterated value. Lemma \@ref(lem:lem31) connects them with $\Vert X_{k}-\boldsymbol1\bar x_{k}\Vert_2$ and $\Vert\bar x_{k+1}-x^*\Vert_2$ and add conditions on $f_i$ and $\alpha'$.

```{lemma, lem31}
Given $f_i, i=1,2,...,n$ are $\mu$-strongly convex and $L$ Lipschitz continuous, 
$$
\left\|\bar{y}_{k}-g_{k}\right\|_{2} \leq \frac{L}{\sqrt{n}}\left\|X_{k}-\mathbf{1} \bar{x}_{k}\right\|_{2}, \quad\left\|g_{k}\right\|_{2} \leq L\left\|\bar{x}_{k}-x^{*}\right\|_{2}
$$
In addition, when $\alpha'\leq \frac{2}{\mu+L}$, we have 
$$
\left\|\bar{x}_{k}-\alpha^{\prime} g_{k}-x^{*}\right\|_{2} \leq\left(1-\alpha^{\prime} \mu\right)\left\|\bar{x}_{k}-x^{*}\right\|_{2}, \quad \forall k
$$
```

However, notice that our final goal involves norm $\Vert\cdot\Vert_R$ and $\Vert\cdot\Vert_C$. We need to transform them, which is ensured from the equivalence of norms. To make the notation more easily, the author gives lemma \@ref(lem:lem35).

```{lemma, lem35}
$\exists \delta_{\mathrm{C}, \mathrm{R}}, \delta_{\mathrm{C}, 2}, \delta_{\mathrm{R}, \mathrm{C}}, \delta_{\mathrm{R}, 2}>0,s.t. \forall X\in\mathbb{R}^{n\times p}$, we have 
$\Vert X\Vert_{\mathrm{C}} \leq \delta_{\mathrm{C}, \mathrm{R}}\Vert X\Vert_{\mathrm{R}},\Vert X\Vert_{\mathrm{C}} \leq \delta_{\mathrm{C}, 2}\Vert X\Vert_{2},\Vert X\Vert_{\mathrm{R}} \leq \delta_{\mathrm{R}, \mathrm{C}}\Vert X\Vert_{\mathrm{C}}$, and 
$\|X\|_{\mathrm{R}} \leq\delta_{\mathrm{R}, 2}\Vert X\Vert_{2}$. In addition, with a proper rescaling of the norms $\Vert\cdot\Vert_R$ and $\Vert\cdot\Vert_C$, we have $\Vert X\Vert_{2} \leq\Vert X\Vert_{\mathrm{R}} \text { and }\Vert X\Vert_{2} \leq\Vert X\Vert_{\mathrm{C}}$

```


On the other hand, 
$\boldsymbol\alpha=\text{diag}(\alpha_1,...,\alpha_n)\in \mathbb{R}^{n\times n}$, then $\Vert \boldsymbol\alpha\Vert_2=\sigma_{\max}(\boldsymbol\alpha)=\underset{i}{\max}\alpha_i:=\hat\alpha$,since $\alpha_i\in\mathbb{R}^+,i=1,2,...,n$. $\sigma(A)$ denotes the singular value of $A$.

Finally, \@ref(eq:ineq11) can be written as 
\begin{equation}

\Vert\bar x_{k+1}-x^*\Vert_2\leq
\left(1-\alpha^{\prime} \mu\right)\left\|\bar{x}_{k}-x^{*}\right\|_{2}+\frac{\alpha^{\prime} L}{\sqrt{n}}\left\|X_{k}-\mathbf{1} \bar{x}_{k}\right\|_{\mathrm{R}}+\frac{\hat{\alpha}\|u\|_{2}}{n}\left\|Y_{k}-v \bar{y}_{k}\right\|_{\mathrm{C}}
(\#eq:ineq1pp)
\end{equation}
Where the first and second parts come from lemma \@ref(lem:lem31), which adds constraints on $f_i$ and $\alpha'$. The second part also uses lemma \@ref(lem:lem35) in transforming different norms, as well as the last part. Additionally, the last part uses lemma \@ref(lem:lem34) when separating $u,\boldsymbol\alpha$ out of norm, which can be seen as a further result of consistency of norms.

```{lemma, lem34}

Given an arbitrary norm $\Vert\cdot\Vert$, $\forall W\in\mathbb{R}^{n\times n}$ and $X\in\mathbb{R}^{n\times p}$, we have $\Vert WX\Vert\leq\Vert W\vert\Vert X\Vert$. $\forall w\in\mathbb{R}^{n\times 1},x\in\mathbb{R}^{1\times p},\Vert wx\Vert = \Vert w\Vert \Vert x\Vert_2$
```

\\

For $\Vert X_{k+1}-\boldsymbol1\bar x_{k+1}\Vert_R$, from \@ref(eq:pp1bar), we have 
\begin{align}
\Vert X_{k+1}-\boldsymbol1\bar x_{k+1}\Vert_R&\leq 
\underbrace{\Vert R-\frac{\mathbf{1} u^{T}}{n}\Vert_R}_{\sigma_R}\cdot\Vert X_{k}-\mathbf{1} \bar{x}_{k}\Vert_R+\Vert R-\frac{\mathbf{1} u^{T}}{n}\Vert_R\cdot \Vert\boldsymbol{\alpha}\Vert_R\cdot\Vert Y_{k}-v\bar y_k+v\bar y_k)\Vert_R\\
&\leq \sigma_R\Vert X_{k}-\mathbf{1} \bar{x}_{k}\Vert_R + 
\sigma_R\Vert \boldsymbol\alpha\Vert_2(\delta_{R,C}\Vert Y_{k}-v\bar y_k\Vert_C + \Vert v\Vert_R\cdot \Vert \bar y_k\Vert_2)\\
&\leq \sigma_R\Vert X_{k}-\mathbf{1} \bar{x}_{k}\Vert_R + 
\sigma_R\Vert \alpha'\Vert_2[\delta_{R,C}\Vert Y_{k}-v\bar y_k\Vert_C + \Vert v\Vert_R (\frac{L}{\sqrt{n}}\left\|X_{k}-\mathbf{1} \bar{x}_{k}\right\|_{2}+L\left\|\bar{x}_{k}-x^{*}\right\|_{2})]\\
&\leq \sigma_R\left(1+\hat{\alpha}\|v\|_{\mathrm{R}} \frac{L}{\sqrt{n}}\right)\left\|X_{k}-\mathbf{1} \bar{x}_{k}\right\|_{\mathrm{R}} + 
\hat{\alpha} \sigma_{\mathrm{R}} \delta_{\mathrm{R}, \mathrm{C}}\left\|Y_{k}-v \bar{y}_{k}\right\|_{\mathrm{C}}+
\hat{\alpha} \sigma_{\mathrm{R}}\|v\|_{\mathrm{R}} L\left\|\bar{x}_{k}-x^{*}\right\|_{2}
(\#eq:ineq2pp)
\end{align}

Where the second inquality is derived from lemma \@ref(lem:lem33) in transforming $\Vert\boldsymbol\alpha\Vert_R=\Vert\boldsymbol\alpha\Vert_2=\hat\alpha$ since $\boldsymbol\alpha$ is diagonal and lemma \@ref(lem:lem35) in transforming $\Vert\cdot\Vert_R$ into $\Vert\cdot\Vert_C$. Next we use lemma \@ref(lem:lem34) and \@ref(lem:lem31) to transform $\Vert \bar y_k\Vert_R$ into the two parts. Finally, we choose a proper rescaling of the norm $\Vert\cdot\Vert_R$ to derive $\Vert X_{k}-\mathbf{1} \bar{x}_{k}\Vert_2\leq\Vert X_{k}-\mathbf{1} \bar{x}_{k}\Vert_R$.   

```{lemma, lem33}
There exist matrix norms $\Vert\cdot\Vert_R$ and $\Vert\cdot\Vert_C$ such that $\sigma_R:=\Vert R-\frac{\mathbf1u^T}{n}\Vert_R<1,\sigma_{\mathrm{C}}:=\left\|\mathbf{C}-\frac{v \mathbf{1}^{\mathrm{T}}}{n}\right\|_{\mathrm{C}}<1$, and $\sigma_R$ and $\sigma_C$ are arbitrarily close to $\rho_R$ and $\rho_C$, respectively. In addition, given any diagnal matrix $W\in\mathbb{R}^{n\times n}$, we have $\|\mathbf{W}\|_{\mathrm{R}}=\|\mathbf{W}\|_{\mathrm{C}}=\|\mathbf{W}\|_{2}$.
```
\\

For $\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C$, denote $\sigma_{\mathrm{C}}:=\left\|\mathbf{C}-\frac{v \mathbf{1}^{\mathrm{T}}}{n}\right\|_{\mathrm{C}}$ and $c_0 :=\Vert I-\frac{v\boldsymbol1^T}{n}\Vert_C$, from \@ref(eq:ppvbar), we have 

\begin{align}
\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C&\leq
\sigma_C\Vert Y_k-v\bar y_k\Vert_C+c_0\Vert\nabla F(X_{k+1})-\nabla F(X_k)\Vert_C\\
&\leq \sigma_C \Vert Y_k-v\bar y_k\Vert_C + c_0L\delta_{C,2}\Vert X_{k+1} - X_k\Vert_2
\end{align}

For $\Vert X_{k+1}-X_k\Vert_2$, we have 
\begin{align}
\Vert X_{k+1}-X_k\Vert_2 &=\Vert R(X_k-\boldsymbol\alpha Y_k)-X_k\Vert_2\\
&=\Vert (R-I)(X_k-\mathbf 1\bar x_k)+R\boldsymbol\alpha (Y_k-v\bar y_k+v\bar y_k)\Vert_2\\
&\leq \Vert R- I\Vert_2\cdot \Vert X_k-\mathbf 1\bar x_k\Vert_R + 
\Vert R\Vert_2\hat\alpha(\Vert Y_k-v\bar y_k+v\bar y_k\Vert_2)\\
\end{align}

The inequality is based on lemma \@ref(lem:lem35) by choosing a proper rescaling of $\Vert \cdot\Vert_R$. Then by lemma \@ref(lem:lem31) and unite like terms, we have 

\begin{align}
\Vert Y_{k+1}-v\bar y_{k+1}\Vert_C&\leq \left(\sigma_{\mathrm{C}}+\hat{\alpha} c_{0} \delta_{\mathrm{C}, 2}\|\mathbf{R}\|_{2} L\right)\left\|Y_{k}-v \bar{y}_{k}\right\|_{\mathrm{C}} +\\ 
&c_{0} \delta_{\mathrm{C}, 2} L\left(\|\mathbf{R}-\mathbf{I}\|_{2}+\hat{\alpha}\|\mathbf{R}\|_{2}\|v\|_{2} \frac{L}{\sqrt{n}}\right)\left\|X_{k}-\mathbf{1} \bar{x}_{k}\right\|_{\mathrm{R}} + 
\hat{\alpha} c_{0} \delta_{\mathrm{C}, 2}\|\mathbf{R}\|_{2}\|v\|_{2} L^{2}\left\|\bar{x}_{k}-x^{*}\right\|_{2}
(\#eq:ineq3pp)
\end{align}

### Spectral radius of A

Lemma \@ref(lem:lem37) lead us to give conditions on $A$ so that $\rho(A)<1$. 

```{lemma, lem37}
Given a nonnegative, irreducible matrix $M=(m_{ij})\in \mathbb{R}^{n\times n}$ with $m_{ii}<\lambda, i=1,2,3$ for some $\lambda>0$. $\rho(M)<\lambda\Leftrightarrow \text{det}(\lambda I-M)>0$
  
```



# Reference {-}