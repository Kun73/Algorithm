---
title: "Optimization"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

This note provides a summary of *Optimization Methods for Large-Scale Machine Learning*, which gives a review and commentary  on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. 

# Objective Function {#sec:obj}

We discuss the supervised learning. The linear regression is a good example to begin with.

For the linear regression, 
\begin{equation}
y_i=X\beta+\epsilon_i,i=1,2,...,n \\
\epsilon_i\stackrel{i.i.d.}\sim N(0,\sigma)
(\#eq:LR)
\end{equation}
To solve \@ref(eq:LR), we use least squares approach to solve the optimization problem, 
\begin{equation}
\beta^*=\frac{1}{2n}\arg\min ||Y-X\beta||^2\\
Y=(y_1,...,y_n)^T
(\#eq:LS)
\end{equation}
In \@ref(eq:LS), we use the $\ell_2$-norm as the loss function $\ell:\mathbb{R}^{d_x}\times\mathbb{R}^{d_y}\rightarrow\mathbb{R}$,
$$
\ell(X\beta,Y)=\frac{1}{2n}||Y-X\beta||^2
$$
and restrict our prediction function $h:\mathbb{R}^{d_x}\times\mathbb{R}^d\rightarrow\mathbb{R}^{d_y}$ to have the linear form 
$$
h(X,\beta)=X\beta
$$
Similar to the linear regression example, we can define a more generic family of prediction function $\mathcal{H}$, a loss function $\ell$, an expected risk $R$, and an empirical risk $R_n$. 
$$
\mathcal{H}:=\left\{h(\cdot ; w): w \in \mathbb{R}^{d}\right\}
$$
Where $w\in\mathbb{R}^d$ is the parameter vector, i.e. the $\beta$ in \@ref(eq:LR). Additinally, we assume that the input-output space $\mathbb{R}^{d_{x}} \times \mathbb{R}^{d_{y}}$ is endowed with a distribution function $P:\mathbb{R}^{d_{x}} \times \mathbb{R}^{d_{y}}\rightarrow[0,1]$, then we can define the **expected risk**
\begin{equation}
R(w)=\int_{\mathbb{R}^{d_{x}} \times \mathbb{R}^{d_{y}}} \ell(h(x ; w), y) d P(x, y)=\mathbb{E}[\ell(h(x ; w), y)]
(\#eq:expr)
\end{equation}

However, we do not have completed information about $P$. In practice, we use the **empirical risk**, 
\begin{equation}
R_{n}(w)=\frac{1}{n} \sum_{i=1}^{n} \ell\left(h\left(x_{i} ; w\right), y_{i}\right)
(\#eq:empr)
\end{equation}

To make the above notation more simple, we introduce the composite function $f(w,\xi)=\ell(h(x;w),y)$, where $\xi$ is the realization of  a sample $(x,y)$. Then our objective function $F:\mathbb{R}^d\rightarrow\mathbb{R}$ can either be the expected risk \@ref(eq:expr) or the empirical risk \@ref(eq:empr), i.e.
\begin{equation}
F(w)=
\begin{cases}
R(w)=E[f(w;\xi)],\\
\text{or}\\
R_n(w)=\frac{1}{n}\sum_{i=1}^nf(w;\xi_i):=\frac{1}{n}\sum_{i=1}^nf_i(w)
\end{cases}
(\#eq:F)
\end{equation}

Our goal, under the *big data* scenario with an infinite supply of training samples, but a limited computational time budget $\mathcal{T}_{\max}$, can be summarized as 
\begin{equation}
w^0 = \underset{\mathcal{H}}{\arg\min}F(w)
(\#eq:obj)
\end{equation}

To compare two algorithms under the *big data* scenario, we are to balance the trade-off among three aspects in \@ref(eq:trade-off). Suppose that both $R$ and $R_n$ attain their minima with parameter vector $w_*=\arg\min R(w)$ and $w_n=\arg\min R_n(w)$, and let $\tilde{w}_n$ be the approximate empirical risk minimizer returned by a given algorithm when $\mathcal{T}_\max$ is exhausted. Then we decomposite the expected risk given $\tilde{w}_n$, $E[R(\tilde{w}_n)]$ as following, 
\begin{equation}
\mathbb{E}\left[R\left(\tilde{w}_{n}\right)\right]=\underbrace{R\left(w_{*}\right)}_{\mathcal{E}_{a p p}(\mathcal{H})}+\underbrace{\mathbb{E}\left[R\left(w_{n}\right)-R\left(w_{*}\right)\right]}_{\mathcal{E}_{e s t}(\mathcal{H}, n)}+\underbrace{\mathbb{E}\left[R\left(\tilde{w}_{n}\right)-R\left(w_{n}\right)\right]}_{\mathcal{E}_{o p t}(\mathcal{H}, n, \epsilon)}
(\#eq:trade-off)
\end{equation}


# Batch Gradient Descent and Stochastic Gradient Descent {#sec:GD}

The idea of gradient descent is based on the fact that $f(x)$ decreases fastest at the direction of gradient in a neighborhood of a point. The iteration scheme is, 
\begin{equation}
x_{n+1} = x_n - \alpha_n I \nabla f(x_n)
(\#eq:GDO)
\end{equation}
where $I$ is the identity matrix, $\alpha_n$ is the steplength. Moreover, $\alpha_n I$ can be seen as an approximation of the hessian matrix $H$ of $f(x)$. This is can be showed onece we give the scheme of Newton-Raphson method,
\begin{equation}
x_{n+1} = x_n - (H(x_n))^{-1}\nabla f(x_n)
(\#eq:nr)
\end{equation}

In summary, the batch gradient descent evaluates the gradient using all the $n$ samples while the stochastic gradient descent(SGD) uses only a subset of all the observations. As the name of SGD, we derive the subset by sampling from the whole observations. To emphasize the need for stpchastic gradient descent under the big data scenario, we again use the linear regression \@ref(eq:LR) as an example. For the optimization problem \@ref(eq:LS), the gradient is 
\begin{align}
\nabla \ell &= (\frac{\partial}{\partial\beta_j}\ell)\\
&=(-\frac{1}{n}\sum_{i=1}^n(y_i-X\beta)x_1, ..., -\frac{1}{n}\sum_{i=1}^n(y_i-X\beta)x_d)^T\in\mathbf{R}^d
(\#eq:glr)
\end{align}

Then we can see from \@ref(eq:glr) that evluating the gradient when $n$ is large(under the big data senario, it is common) costs a lot. For this reason, SGD is better. We then give the pseudocode of SGD.

---

**(stochastic gradient descent(SGD))**

1. Choose a starting point $w_1$
2. **For** k = 1, 2, ...
   - Generate a realization of the random variable $\xi_k$
   
   - Compute a stochastic vector $g(w_k,\xi_k)$
   
   - Choose a steplength $\alpha_k>0$

   - Set the new point $w_{k+1}<- w_k - \alpha_k g(w_k,\xi_k)$

---

The stochastic vector $g(w_k,\xi_k)$ can represents the following choices
\begin{equation}
g(w_k,\xi_k) = 
\begin{cases}
\nabla f(w_k;\xi_k)\\
\frac{1}{n_k}\sum_{i=1}^{n_k}\nabla f(w_k;\xi_{k,i})\\
H_k\frac{1}{n_k}\sum_{i=1}^{n_k}\nabla f(w_k;\xi_{k,i})
\end{cases}
(\#eq:sv)
\end{equation}

$H_k$ is a symmetric positive definite(p.d.) scaling matrix. All the following theretical results hold as long as the expected angle between $g(w_k,\xi_k)$ and $\nabla F(w_k)$ is sufficiently positive. For the choice of stepsize sequence $\{\alpha_k\}$. We focus on two choices, 

- a fixed stepsize, and 

- diminshing stepsizes, e.g. 
\begin{equation}
\sum_{i=1}^\infty \alpha_i = \infty, \sum_{i=1}^\infty\alpha_i^2 <\infty
(\#eq:dimishingalpha)
\end{equation}

## Convergence of SG

A straightforward approach for establishing convergence guarantees for SG is to assume some smoothness condistions on the objective function.

```{definition, name = "Lipschitz continuous"}
A function $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$ is called *Lipschitz-continuous* with *Lipschitz constant* $L>0$, if
\begin{equation}
||f(x_1)-f(x_2)||_2\leq L||x_1-x_2||_2,\quad \forall x_1,x_2\in \mathbb{R}^m
\end{equation}
```

Our first assumption for the smoothness of the objective function $F$ is to assume that $F:\mathbb{R}^d\rightarrow \mathbb{R}$ is continuously differentiable and moreover, the gradient of $F$, $\nabla F:\mathbb{R}^d\rightarrow\mathbb{R}^d$ is Lipschitz continuous with Lipschitz constant $L>0$, i.e. 
\begin{equation}
\|\nabla F(w)-\nabla F(\bar{w})\|_{2} \leq L\|w-\bar{w}\|_{2} \text { for all }\{w, \bar{w}\} \subset \mathbb{R}^{d}
(\#eq:ass1)
\end{equation}

We can derive *uniformly continuous* from assumption \@ref(eq:ass1), thus it ensures $\nabla F$ does not change arbitraily quickly with respect to the parameter vector. By Taylor series, we can expand $F$ at $w=\bar{w}$, 
\begin{align}
F(w) &= F(\bar{w}) + \nabla F(\bar{w})^T(w-\bar{w})+\frac{H}{2}||w-\bar{w}||_2+o(||w-\bar{w}||_2)\\
&\leq F(\bar{w}) + \nabla F(\bar{w})^T(w-\bar{w}) + \frac{L}{2}||w-\bar{w}||_2
(\#eq:con1ass1)
\end{align}
The inequality can be derived directly by assumption \@ref(eq:ass1). 



# Acknowledgment {-}

The above contents are notes from *Optimization Methods for Large-Scale Machine Learning* by @bottou2018optimization.

# Reference {-}
