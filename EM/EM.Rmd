---
title: "EM Algorithm"
author: "Kun Huang"
date: "`r format(Sys.Date())`"
output: bookdown::html_document2
bibliography: template.bib
biblio-style: asa
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


The EM algorithm stands for expectation-maximization algorithm. It is a special case of MM(minorization-maximization) algorithm.

# MM Alogrithm {#sec:MM}

Our goal is to solve the problem \@ref(eq:obj),
\begin{equation}
x^* = \arg\max f(x)
(\#eq:obj)
\end{equation}

From its name, the MM algorithm has two steps at each iteration $x_{t}$,

- Minorization: Find a surrogate function $g_t(x), s.t.$

\begin{equation}
g_t(x)\leq f(x),
(\#eq:wh)
\end{equation}

\begin{align}
g_t(x_t) = f(x_t)
(\#eq:loc)
\end{align}

- Maximization: Maximize the surrogate function $g_t(x)$

\begin{equation}
x_{t+1} = \arg\max g_t(x)
(\#eq:max)
\end{equation}

Thus at each iteration, we have 
\begin{equation}
f(x_{t+1})\geq g_t(x_{t+1})\geq g_t(x_t) = f(x_t)
(\#eq:con)
\end{equation}

The inequality \@ref(eq:con) holds because of \@ref(eq:wh), \@ref(eq:max) and \@ref(eq:loc) respectively. 

Since our optimization is on the surrogate function $g_t(x)$, we can avoid dealing with some unpleasant properties in $f(x)$. However, finding such a surrogate function is not easy and does not have universal method that it varys in different problems. 

The EM algorithm can be seen as a special case of MM algorithm. 


# EM Algorithm

The EM algorithm was proposed by @EM. It is widely used dealing with incomlete data set. For example, in the Gaussian mixture regression model(see \@ref(sec:gmm)), the observations are not from homogeneous group. 

For the following discussions, we always assume $\mathbf{Y}=\mathbf{y}$ is the observed data which is fixed and known from space $\Omega(Y)$, $\mathbf{X} = \mathbf{x}$ is the complete data but not observed from space $\Omega(X)$. Additionally, there exists a many-to-one mapping $\mathbf{x}->\mathbf{y}(\mathbf{x})$ from $\Omega(X)$ to $\Omega(Y)$, and that $\mathbf{x}$ is known only to lie in $M(\mathbf{y})$, the subset determined by the equation $\mathbf{y} = \mathbf{y}(\mathbf{x})$.  We postulate a family of sampling density $f(\mathbf{x}|\Phi)$ depending on parameters $\Phi$ and derive its corresponding family of sampling density $g(\mathbf{y}|\Phi)$ by 
\begin{equation}
g(\mathbf{y}|\Phi) = \int_{M(\mathbf{y})} f(\mathbf{x}|\Phi) d\mathbf{x}
(\#eq:int)
\end{equation}

Our goal is to get the MLE of $\Phi$ from the log-likelihood function $\ell(\Phi|\mathbf{y})$ when an observed data $\mathbf{y}$ is given. The EM algorithm does so by **making essential use of the associated family ** $f(\mathbf{x}|\Phi)$. 

## General EM Algorithm {#sec:gen}

To get the MLE of $\Phi$ from $\ell(\Phi|\mathbf{y})$ given an observed data $\mathbf{y}$, we aim to derive a $\Phi^*$ that maximize $\ell(\Phi|\mathbf{y})$. 

First, $\ell(\Phi|\mathbf{y})$ can be represented by $f(\mathbf{x}|\Phi)$ and a corresponding conditional density $k(\mathbf{x}|\mathbf{y},\Phi)$, i.e.
\begin{align}
(\#eq:conell)
\ell(\Phi|\mathbf{y}) &= \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\mathbf{y},\Phi)\\

k(\mathbf{x}|\mathbf{y},\Phi) &= \frac{f(\mathbf{x}|\Phi)}{g(\mathbf{y}|\Phi)}
\end{align}
Where $k(\mathbf{x}|\mathbf{y},\Phi)$ depends only on $\mathbf{x}$ when $\mathbf{y}$ is given, we denote it as $k(\mathbf{x}|\Phi)$, then \@ref(eq:conell) becomes 

\begin{equation}
\ell(\Phi|\mathbf{y}) = \log f(\mathbf{x}|\Phi) - \log k(\mathbf{x}|\Phi)
(\#eq:ellbefore)
\end{equation}

For \@ref(eq:ellbefore), we integrate (or sum) both sides after multiplying with $k(\mathbf{x}|\Phi^{(t)})$. The LHS has nothing random with  respetive to $\mathbf{x}$. For the RHS, we have 
\begin{align}

\int_{M(\mathbf{y})}\log (f(\mathbf{x}|\Phi))k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}
&=E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]:= Q(\Phi|\Phi^{(t)}) \\
\int_{M(\mathbf{y})}\log( k(\mathbf{x}|\Phi))k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}&:=H(\Phi|\Phi^{(t)})
\end{align}

Thus we have 
\begin{equation}
\ell(\Phi|\mathbf{y}) = Q(\Phi|\Phi^{(t)}) - H(\Phi|\Phi^{(t)})
(\#eq:emfin)
\end{equation}

Where $\Phi^{(t)}$ is the parameter $\Phi$ aftering iterating $t$ times. According to \@ref(eq:emfin), we update $\ell(\Phi|\mathbf{y})$ by repeatedly updating $Q(\Phi|\Phi^{(t)})$ and $H(\Phi|\Phi^{(t)})$. We now give the E- and M-step of EM algorithm.

- E-step: Compute the conditional expectation,
$$
Q(\Phi|\Phi^{(t)}) = E[\log f(\mathbf{x}|\Phi)| \mathbf{y},\Phi^{(t)}]
$$ 

- M-step: Derive the next $\Phi^{(t+1)}$ by maximizing $Q(\Phi|\Phi^{(t)})$, i.e. 
\begin{equation}
(\#eq:mgen)
\Phi^{(t+1)} = \arg\max Q(\Phi|\Phi^{(t)})
\end{equation}

Note that $-H(\Phi|\Phi^{(t)})$ can be seen as the entropy of $k(\mathbf{x}|\Phi)$, which is non-negative, since $k(\mathbf{x}|\Phi)\in[0,1]$. 

So $\ell(\Phi|\mathbf{y})\geq Q(\Phi|\Phi')$. Thus $Q(\Phi|\Phi')$ is the minorization of $\ell(\Phi|\mathbf{y})$. Hence the EM algorithm leads to the maximal according to section \@ref(sec:MM). 

Also, we can prove the convergence of EM algorithm by showing that updating $\Phi^{(t)}$ by the E- and M-steps leads to ultimately maximal of $\ell(\Phi|\mathbf{y})$, i.e. we show $\ell(\Phi^{(t+1)}|\mathbf{y})\geq\ell(\Phi^{(t)}|\mathbf{y})$.

\begin{align}
\ell(\Phi^{(t+1)}|\mathbf{y}) - \ell(\Phi^{(t)}|\mathbf{y})
& = \underbrace{(Q(\Phi^{(t+1)}|\Phi^{(t)}) - Q(\Phi^{(t)}|\Phi^{(t)}))}_{\text{M-step, }\geq0}+\underbrace{(H(\Phi^{(t)}|\Phi^{(t)}) - H(\phi^{(t+1)}|\Phi^{(t)}))}_{\text{Jensen's inequality}, f(\cdot) = \log(\cdot),\geq0}\\
&\geq 0
\end{align}


## EM Algorithm for Exponential Distribution

In this section, we restrict the complete density $f(\mathbf{x}|\Phi)$ has regular($\Phi$ is unique up to an arbitary non-singular $r\times r$ linear transformation) exonential-family form
\begin{equation}
(\#eq:exp)
f(\mathbf{x}|\Phi) = b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi/a(\Phi)\\
\Phi, \mathbf{t}(\mathbf{x})\in\mathbb{R}^r
\end{equation}
Here, $\mathbf{t}(\mathbf{x})$ is the sufficient statistics. 

From the above discussion \@ref(eq:emfin),Substituting $f(\mathbf{x}|\Phi)$, we have 
\begin{align}
k(\mathbf{x}|\mathbf{y},\Phi) = \frac{b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi)}{\int_{M(\mathbf{y})} b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi d\mathbf{x}}
:=\frac{b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x}))^T\Phi)}{a(\Phi|\mathbf{y})}
\end{align}

Thus 
\begin{align}
Q(\Phi|\Phi^{(t)}) &= \int_{M(y)}\log b(\mathbf{x})k(\mathbf{x}|\Phi^{(t)})d\mathbf{x} + \int_{M(y)}\mathbf{t}(\mathbf{x})^T\Phi k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}-\int_{M(y)} \log (a(\Phi)) k(\mathbf{x}|\Phi^{(t)})d\mathbf{x}\\
&:=B(\Phi^{(t)}) + \Phi^T E[\mathbf{t}|\Phi^{(t)},\mathbf{y}] - \log a(\Phi)

\end{align}

Let $Q'(\Phi|\Phi^{(t)})=E[\mathbf{t}|\Phi^{(t)},\mathbf{y}]-\frac{a'(\Phi)}{a(\Phi)} = 0$, notice that $\int f(\mathbf{x}|\Phi)d\mathbf{x}=1$, then
\begin{align}
a(\Phi) &= \int b(\mathbf{x})\exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x} \stackrel{\text{take ln then take derivative}}\Rightarrow\\
\frac{a'(\Phi)}{a(\Phi)} &= \frac{\int b(\mathbf{x}) \mathbf{t}(\mathbf{x})^T \exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x}}{\int b(\mathbf{x})  \exp(\mathbf{t}(\mathbf{x})^T\Phi)d\mathbf{x}}
= E[\mathbf{t}(\mathbf{x})|\Phi,\mathbf{y}]
\end{align}

Hence we derive the EM algorithm for exponential family as follows

- E-step: Compute $E[\mathbf{t}(\mathbf{x})|\phi^{(t)},\mathbf{y}]$

- M-step: Update $\Phi^{(t+1)}$ by solving $E[\mathbf{t}(\mathbf{x})|\phi^{(t+1)},\mathbf{y}]=E[\mathbf{t}(\mathbf{x})|\phi^{(t)},\mathbf{y}]$

## Gaussian Mixture Model {#sec:gmm}



For a Gaussian mixture model, we have 
\begin{equation}
y_i = \mathbf{c}_i | (\mathbf{X} = \mathbf{x}_i,\mathbf{Z}=\mathbf{z}_i)
(\#eq:gmmdata)
\end{equation}
where $\mathbf{c}_i$ is the complete observation which cannot be observed directly, $\mathbf{x}_i$ is the predictor, $\mathbf{z}_i = \left(z_{i 1}, z_{i 2}, \ldots, z_{i m}\right)$ is the latent variable indicating which group $y_i$ belongs to. 

Suppose the density of $y_i$ is given by 
\begin{equation}
f\left(y_{i} | \mathbf{x}_{i}, \mathbf{\Phi}\right)=\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i} ; \mathbf{x}_{i}^{\top} \boldsymbol{\beta}_{j}, \sigma^{2}\right), \quad i=1, \ldots, n
(\#eq:gmmden)
\end{equation}
Where $\pi_j's$ are called mixing parameters, $\boldsymbol{\beta}_j$ is the regression coefficient vector for the $j$th group, $\phi\left(y_{i} ; \mu, \sigma^{2}\right)$  denotes the density function of $\mathcal{N}(\mu,\sigma^2)$, $\boldsymbol{\Phi}=(\pi_1,\boldsymbol{\beta}_1,...,\pi_m,\boldsymbol{\beta}_m,\sigma)$ contains all the unknown parameters.

The log-likelihood function $\ell(\boldsymbol{\Phi})$ of observed data can be represented as 
\begin{equation}
\ell(\boldsymbol{\Phi}) = \sum_{i=1}^n\log \left[\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i} ; \mathbf{x}_{i}^{\top} \boldsymbol{\beta}_{j}, \sigma^{2}\right)\right]
(\#eq:gmmell)
\end{equation}


## Variants of EM

### Monte Carlo EM(MCEM)

### Generalized EM(GEM)

## Acceleration of EM


# Acknowledgment {-}

The above contents are notes after taking the class from Prof. @stat5361.

# Reference {-}
